{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alejg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version 2\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import torch\n",
    "nltk.download('punkt')\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "import torch.optim as optim\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path_json=\"D:/datos/annotations_trainval2017/annotations/captions_train2017.json\"\n",
    "training_path_images=\"D:/datos/train2017/train2017/\"\n",
    "validation_path_json=\"C:/Users/alejg/Documents/proyectos_personales/images_segmentation/coco/annotations/captions_val2014.json\"\n",
    "validation_path_images=\"C:/Users/alejg/Documents/proyectos_personales/images_segmentation/coco/images/val2014/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 400#400<-          # batch size\n",
    "vocab_threshold = 4 #<-      # minimum word count threshold\n",
    "vocab_from_file = False  #<-  # if True, load existing vocab file\n",
    "embed_size =   256#<-         # dimensionality of image and word embeddings\n",
    "hidden_size = 512#<-         # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 10             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "epoch_ini=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.76s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/591753] Tokenizing captions...\n",
      "[100000/591753] Tokenizing captions...\n",
      "[200000/591753] Tokenizing captions...\n",
      "[300000/591753] Tokenizing captions...\n",
      "[400000/591753] Tokenizing captions...\n",
      "[500000/591753] Tokenizing captions...\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                        | 1205/591753 [00:00<00:49, 11930.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.75s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 591753/591753 [00:46<00:00, 12859.44it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file,\n",
    "                             images_path=training_path_images,annotation_path=training_path_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                                        | 2504/202654 [00:00<00:16, 12450.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.29s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 202654/202654 [00:16<00:00, 12553.67it/s]\n"
     ]
    }
   ],
   "source": [
    "validation_dataloader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=10,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=True,\n",
    "                                  images_path=validation_path_images,annotation_path=validation_path_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the vocabulary.\n",
    "vocab_size = len(train_dataloader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderRNN(\n",
       "  (word_embeddings): Embedding(11543, 256)\n",
       "  (lstm): LSTM(256, 512, batch_first=True)\n",
       "  (linear): Linear(in_features=512, out_features=11543, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11543"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 256#<-\n",
    "hidden_size = 512#<-\n",
    "vocab_size = 11543#9999\n",
    "\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_ini= 9\n",
      "loaded\n"
     ]
    }
   ],
   "source": [
    "print(f\"epoch_ini= {epoch_ini}\")\n",
    "if epoch_ini != 0:\n",
    "    # Load pretrained models\n",
    "    \n",
    "    encoder.load_state_dict(torch.load(\"saved_models/v3encoder_%d.pkl\" % epoch_ini))\n",
    "    decoder.load_state_dict(torch.load(\"saved_models/v3decoder_%d.pkl\" % epoch_ini))\n",
    "    epoch_ini=epoch_ini+1\n",
    "    print(\"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = list(decoder.parameters())  + list(encoder.parameters())\n",
    "params =list(filter(lambda p: p.requires_grad,decoder.parameters()))+list(filter(lambda p: p.requires_grad,encoder.parameters()))\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = optim.Adam(params, lr=0.001, betas=(0.9, 0.999))#optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(train_dataloader.dataset.caption_lengths) / train_dataloader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#tensorboard --logdir=C:/Users/alejg/Documents/tfm/image_captioning_backend/train/tensroboar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open the training log file.\n",
    "#f = open(log_file, 'w')\n",
    "#\n",
    "#for epoch in range(epoch_ini, num_epochs):\n",
    "#    \n",
    "#    for i_step in range(1, total_step+1):\n",
    "#        # Randomly sample a caption length, and sample indices with that length.\n",
    "#        indices = train_dataloader.dataset.get_train_indices()\n",
    "#        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "#        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "#        train_dataloader.batch_sampler.sampler = new_sampler\n",
    "#        # Obtain the batch.\n",
    "#        images, captions = next(iter(train_dataloader))\n",
    "#\n",
    "#        # Move batch of images and captions to GPU if CUDA is available.\n",
    "#        images = images.to(device)\n",
    "#        captions = captions.to(device)\n",
    "#        \n",
    "#        # Zero the gradients.\n",
    "#        decoder.zero_grad()\n",
    "#        encoder.zero_grad()\n",
    "#        \n",
    "#        # Pass the inputs through the CNN-RNN model.\n",
    "#        features = encoder(images)\n",
    "#        outputs = decoder(features, captions)\n",
    "#        \n",
    "#        # Calculate the batch loss.\n",
    "#        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "#        \n",
    "#        # Backward pass.\n",
    "#        loss.backward()\n",
    "#        \n",
    "#        # Update the parameters in the optimizer.\n",
    "#        optimizer.step()\n",
    "#            \n",
    "#        # Get training statistics.\n",
    "#        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "#        \n",
    "#        # Print training statistics (on same line).\n",
    "#        print('\\r' + stats, end=\"\")\n",
    "#        sys.stdout.flush()\n",
    "#        \n",
    "#        # Print training statistics to file.\n",
    "#        f.write(stats + '\\n')\n",
    "#        f.flush()\n",
    "#        \n",
    "#        # Print training statistics (on different line).\n",
    "#        if i_step % print_every == 0:\n",
    "#            print('\\r' + stats)\n",
    "#            \n",
    "#    # Save the weights.\n",
    "#    if epoch % save_every == 0:\n",
    "#        torch.save(decoder.state_dict(), \"saved_models/v3decoder_%d.pkl\" % epoch)\n",
    "#        torch.save(encoder.state_dict(), \"saved_models/v3encoder_%d.pkl\" % epoch)\n",
    "#\n",
    "## Close the training log file.\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader.dataset.caption_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(validation_dataloader.dataset.caption_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi_validation(decoder,encoder, loader, criterion):\n",
    "    test_loss = 0\n",
    "    total_step = math.ceil(len(loader.dataset.caption_lengths) / loader.batch_sampler.batch_size)\n",
    "    vocab_size = len(loader.dataset.vocab)\n",
    "    perplexityAcum=0\n",
    "    perplexityVec=[]\n",
    "    for i_step in tqdm.tqdm(range(1, total_step+1)):\n",
    "        images, captions = next(iter(loader))\n",
    "         # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        perplexity=np.exp(loss.item())\n",
    "        perplexityVec.append(perplexity)\n",
    "        perplexityAcum +=perplexity\n",
    "        test_loss += loss   \n",
    "        \n",
    "    return test_loss,perplexityAcum,np.mean(perplexityVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter('tensroboar')\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "for epoch in range(epoch_ini, num_epochs):\n",
    "    decoder.train()\n",
    "    encoder.train()\n",
    "    for i_step in tqdm.tqdm(range(1, total_step+1)):\n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = train_dataloader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        train_dataloader.batch_sampler.sampler = new_sampler\n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(train_dataloader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    decoder.eval()\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss,train_perplexityAcum,train_perplexityMean = mi_validation(decoder,encoder, train_dataloader, criterion)\n",
    "        \n",
    "    totSteptrain = math.ceil(len(train_dataloader.dataset.caption_lengths) / train_dataloader.batch_sampler.batch_size)\n",
    "    losstrain=train_loss/totSteptrain\n",
    "    print(f\"loss training {losstrain}\")\n",
    "    writer.add_scalar('loss_training', losstrain, epoch)\n",
    "    print(f\"perplexityAcum training {train_perplexityAcum}\")\n",
    "    writer.add_scalar('train_perplexityAcum', train_perplexityAcum, epoch)\n",
    "    print(f\"perplexityMean training {train_perplexityMean}\")\n",
    "    writer.add_scalar('train_perplexityMean', train_perplexityMean, epoch)\n",
    "    \n",
    "    \n",
    "    decoder.eval()\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss,test_perplexityAcum,test_perplexityMean= mi_validation(decoder,encoder, validation_dataloader, criterion)\n",
    "    \n",
    "    totStepValid = math.ceil(len(validation_dataloader.dataset.caption_lengths) / validation_dataloader.batch_sampler.batch_size)\n",
    "    losstest=test_loss/totStepValid\n",
    "    print(f\"loss validation {losstest}\")\n",
    "    writer.add_scalar('loss_validation', losstest, epoch)\n",
    "    print(f\"perplexityAcum validation {test_perplexityAcum}\")\n",
    "    writer.add_scalar('test_perplexityAcum', test_perplexityAcum, epoch)\n",
    "    print(f\"perplexityMean validation {test_perplexityMean}\")\n",
    "    writer.add_scalar('test_perplexityMean', test_perplexityMean, epoch)\n",
    "    \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), \"saved_models/v3decoder_%d.pkl\" % epoch)\n",
    "        torch.save(encoder.state_dict(), \"saved_models/v3encoder_%d.pkl\" % epoch)\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
